{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loginkey\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import re\n",
    "\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class element_not_have_css_class(object):\n",
    "    \"\"\"An expectation for checking that an element has a particular css class.\n",
    "\n",
    "    locator - used to find the element\n",
    "    returns the WebElement once it has the particular css class\n",
    "    \"\"\"\n",
    "    def __init__(self, locator, css_class):\n",
    "        self.locator = locator\n",
    "        self.css_class = css_class\n",
    "\n",
    "    def __call__(self, driver):\n",
    "        element = driver.find_element(*self.locator)   # Finding the referenced element\n",
    "        if self.css_class not in element.get_attribute(\"class\"):\n",
    "            return element\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    \n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    print('initializing driver...')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get('https://www.artsy.net/')\n",
    "    \n",
    "    print('loaded artsy.net, waiting to log in...')\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    time.sleep(2)\n",
    "\n",
    "    button = driver.find_element(By.XPATH, '/html/body/div[2]/div/div/div[1]/div/header/div/nav/div[1]/a[2]')\n",
    "    driver.execute_script(\"arguments[0].click();\", button)\n",
    "\n",
    "    # user = loginkey.user\n",
    "    # password = loginkey.password\n",
    "\n",
    "    user = 'michaeljroth815@gmail.com'\n",
    "    password = 'YudBYiLj$e7u2Xq'\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    print('logging in...')\n",
    "    \n",
    "    wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'hgOWHr')))\n",
    "\n",
    "    driver.find_element(By.CLASS_NAME, 'hgOWHr').click()\n",
    "    driver.find_element(By.XPATH, \"//input[@name='email']\").send_keys(user)\n",
    "    driver.find_element(By.CLASS_NAME, 'jGXstL').click()\n",
    "    driver.find_element(By.XPATH, \"//input[@name='password']\").send_keys(password)\n",
    "    driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "    \n",
    "    print('log in sucessful...')\n",
    "    \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auction_scrape(driver, list_of_artists):\n",
    "    \n",
    "    max_pages = 100\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    auction_results = []\n",
    "    count = 0\n",
    "    \n",
    "    for a, artist in enumerate(list_of_artists):\n",
    "        try:\n",
    "\n",
    "            url = f'https://www.artsy.net/artist/{artist}/auction-results?metric=in'\n",
    "\n",
    "            driver.get(url)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            wait.until(EC.element_to_be_clickable((By.XPATH,'//select')))\n",
    "\n",
    "            select_element = driver.find_elements(By.XPATH,'//select')\n",
    "            for i in range(len(select_element)):\n",
    "                select_object = Select(select_element[i])\n",
    "                try:\n",
    "                    select_object.select_by_value('PRICE_AND_DATE_DESC')\n",
    "                    break\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for i in range(max_pages):\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "                loading_container = soup.find(class_='LoadingArea__Container-sc-1qecphp-2')\n",
    "                title_divs = loading_container.find_all(class_='jvPMUs')\n",
    "                image_divs = loading_container.find_all(class_='fvrqBk')\n",
    "                info_divs = loading_container.find_all(class_='icNtnl')\n",
    "                price_divs = loading_container.find_all(class_='iZqizk')\n",
    "\n",
    "                for i in range(9):\n",
    "                    try:\n",
    "                        title = title_divs[i*2].find_all(class_='ldlHGe')[0].text\n",
    "                    except:\n",
    "                        title = float(\"nan\")\n",
    "                    try:\n",
    "                        image_url = image_divs[i].find('img').get('src')\n",
    "                        pattern = re.compile(\"(http|ftp|https)\")\n",
    "                        find_2nd = pattern.findall(image_url)\n",
    "                        find_http = pattern.split(find_2nd[0][2])\n",
    "                        image_url = ''.join(find_http[1:])\n",
    "                    except:\n",
    "                        image_url = float(\"nan\")\n",
    "                    try:\n",
    "                        medium = info_divs[i].find_all(class_='buodgj')[1].contents[0]\n",
    "                    except:\n",
    "                        medium = float(\"nan\")\n",
    "                    try:\n",
    "                        dimensions = info_divs[i].find_all(class_='buodgj')[1].contents[-1]\n",
    "                    except:\n",
    "                        medium = float(\"nan\")\n",
    "                    try:\n",
    "                        estimate = info_divs[i].find_all(class_='buodgj')[3].contents[0]\n",
    "                    except:\n",
    "                        estimate = float(\"nan\")\n",
    "                    try:\n",
    "                        realized_price = info_divs[i].find_all(class_='buodgj')[7].contents[0]\n",
    "                    except:\n",
    "                        realized_price = float(\"nan\")\n",
    "                    try:\n",
    "                        try:\n",
    "                            price = price_divs[i].find(class_='eHKyyH').text\n",
    "                        except:\n",
    "                            price = price_divs[i].find(class_='ldlHGe').text\n",
    "                        price = int(price.replace('US$', '').replace(',',''))\n",
    "                    except:\n",
    "                        price = None\n",
    "                    result = {\n",
    "                        'artist_slug': artist,\n",
    "                        'title': title,\n",
    "                        'image_url': image_url,\n",
    "                        'medium': medium,\n",
    "                        'dimensions': dimensions,\n",
    "                        'estimate': estimate,\n",
    "                        'realized_price': realized_price,\n",
    "                        'price': price\n",
    "                    }\n",
    "\n",
    "                    auction_results.append(result)\n",
    "                    print(f'{count} total objects scraped, {a+1} / {len(artists)} artists searched', end='\\r')\n",
    "                    count += 1\n",
    "\n",
    "                try:\n",
    "\n",
    "                    driver.find_element(By.LINK_TEXT, 'Next').click()\n",
    "\n",
    "                    element = wait.until(element_not_have_css_class((By.CLASS_NAME, 'beISLe'), \"loading\"))\n",
    "                except:\n",
    "                    break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    driver.close() \n",
    "    \n",
    "    return auction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('artsy_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = df.artist_slug.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4750"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing driver...\n",
      "loaded artsy.net, waiting to log in...\n",
      "logging in...\n",
      "log in sucessful...\n",
      "waiting to scrape...\n",
      "scraping...\n",
      "28871 total objects scraped, 94 / 4750 artists searched\r"
     ]
    }
   ],
   "source": [
    "driver = initialize_driver()\n",
    "print('waiting to scrape...')\n",
    "time.sleep(10)\n",
    "print('scraping...')\n",
    "results = auction_scrape(driver, artists)\n",
    "print('scraping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('medium == \"Painting\"').describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('medium == \"Painting\"').groupby('artist_slug').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('medium == \"Painting\"').dropna(subset=['title']).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query('artist_slug == \"leonardo-da-vinci\"').sort_values('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.log(results_df.price).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get rid of login info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
